---
title: "README"
output: github_document
---

## Data download

We use Trip, Boat, Device, Points, and Catch data. 

Trip, Boat, and Device data should be downloaded manually from [https://analytics.pelagicdata.com/](https://analytics.pelagicdata.com/). 
To match you should filter for MAF/WorldFish as customer and
date after 2019-04-01 which is roughly when catch collection starts

The script `data-download.R` helps download Points and Catch data but manual input is necessary. 
When you run the script, authentication with a Google account with access to the datasets needed. 
Catch data is downloaded automatically with the script but Point data is not. 
Specifically Point data is exported from BigQuery into into a series of files in Google Cloud Storage and then should be downloaded manually. 
One can use gsutil to simplify this

```{bash, eval = F}
gsutil cp \
gs://timor/timor-all_points_2021-03-14_000000000000.csv.gz \
gs://timor/timor-all_points_2021-03-14_000000000001.csv.gz \
gs://timor/timor-all_points_2021-03-14_000000000002.csv.gz \
gs://timor/timor-all_points_2021-03-14_000000000003.csv.gz \
gs://timor/timor-all_points_2021-03-14_000000000004.csv.gz \
gs://timor/timor-all_points_2021-03-14_000000000005.csv.gz \
gs://timor/timor-all_points_2021-03-14_000000000006.csv.gz \
gs://timor/timor-all_points_2021-03-14_000000000007.csv.gz \
gs://timor/timor-all_points_2021-03-14_000000000008.csv.gz \
gs://timor/timor-all_points_2021-03-14_000000000009.csv.gz \
gs://timor/timor-all_points_2021-03-14_000000000010.csv.gz \
gs://timor/timor-all_points_2021-03-14_000000000011.csv.gz \
gs://timor/timor-all_points_2021-03-14_000000000012.csv.gz \
gs://timor/timor-all_points_2021-03-14_000000000013.csv.gz \
gs://timor/timor-all_points_2021-03-14_000000000014.csv.gz \
.
```
